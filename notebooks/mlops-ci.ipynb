{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "already moved\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "try:\n",
        "    os.chdir('Users/giosue.cotugno/mlops_titanic/notebooks/')\n",
        "except:\n",
        "    print('already moved')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1670961745732
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "from azureml.data.data_reference import DataReference\n",
        "from azureml.pipeline.core import Pipeline, PipelineData\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "import os\n",
        "from azureml.core import Workspace,Experiment, Environment\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute import ComputeTarget\n",
        "from azureml.core.compute_target import ComputeTargetException"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Defining utility function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getOrCreateCompute(ws:Workspace):\n",
        "    \n",
        "    \n",
        "\n",
        "    aml_compute_target = \"testcot\"\n",
        "    try:\n",
        "        aml_compute = AmlCompute(ws, aml_compute_target)\n",
        "        print(\"found existing compute target.\")\n",
        "    except ComputeTargetException:\n",
        "        print(\"creating new compute target\")\n",
        "\n",
        "        provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
        "                                                                    min_nodes = 0, \n",
        "                                                                    max_nodes = 4)    \n",
        "        aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
        "        aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
        "    finally:\n",
        "        return aml_compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def createRunConfig(ws):\n",
        "    \n",
        "    from azureml.core.runconfig import RunConfiguration\n",
        "    from azure.ai.ml import MLClient\n",
        "    from azure.identity import DefaultAzureCredential\n",
        "    from azureml.core import Environment\n",
        "    # create a new runconfig object\n",
        "    run_config = RunConfiguration()\n",
        "    env = Environment.get(workspace=ws, name='TITANIC', version='1')\n",
        "\n",
        "    run_config.environment=env\n",
        "\n",
        "    \n",
        "\n",
        "    return run_config"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build dev pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mlops-aml-ws\n",
            "mlops-rg\n",
            "westeurope\n",
            "f90533aa-280d-40b9-9949-a7ba0ee9511f\n",
            "Blobstore's name: workspaceblobstore\n"
          ]
        }
      ],
      "source": [
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
        "\n",
        "\n",
        "# Default datastore (Azure blob storage)\n",
        "def_blob_store = ws.get_default_datastore()\n",
        "#def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
        "print(\"Blobstore's name: {}\".format(def_blob_store.name))\n",
        "#Upload file to datastore\n",
        "\n",
        "# Use a CSV to read in the data set.\n",
        "file_name = \"../data/rawdata/train.csv\"\n",
        "\n",
        "if not os.path.exists(file_name):\n",
        "    raise Exception(\n",
        "        'Could not find CSV dataset at \"%s\". '\n",
        "        % file_name\n",
        "    )  # NOQA: E50\n",
        "# Upload file to default datastore in workspace"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Uploading data to blob storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1670961575665
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "found existing compute target.\n"
          ]
        }
      ],
      "source": [
        "target_path = \"training-data/\"\n",
        "def_blob_store.upload_files(\n",
        "    files=[file_name],\n",
        "    target_path=target_path,\n",
        "    overwrite=True,\n",
        "    show_progress=False,\n",
        ")\n",
        "\n",
        "blob_input_data = DataReference(\n",
        "    datastore=def_blob_store,\n",
        "    data_reference_name=\"test_data\",\n",
        "    path_on_datastore=\"training-data/train.csv\")\n",
        "\n",
        "\n",
        "aml_compute = getOrCreateCompute(ws)\n",
        "run_config = createRunConfig(ws)\n",
        "#processed_data1 = PipelineData(\"processed_data1\",datastore=def_blob_store)\n",
        "models_data = PipelineData(\"models_data\",datastore=def_blob_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1670961575794
        }
      },
      "outputs": [],
      "source": [
        "source_directory=\"../src/preprocess/\"\n",
        "preprocess_step = PythonScriptStep(\n",
        "    \n",
        "    name=\"Preprocessing Step\",\n",
        "    script_name=\"preprocess.py\", \n",
        "    arguments=[\"--data\", blob_input_data],\n",
        "    inputs=[blob_input_data],\n",
        "    compute_target=aml_compute, \n",
        "    source_directory=source_directory,\n",
        "    runconfig=run_config,\n",
        "    allow_reuse=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<azureml.pipeline.core.pipeline.Pipeline at 0x7f473b6f6370>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from azureml.pipeline.core import Pipeline\n",
        "Pipeline(ws, [preprocess_step])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1670961575919
        }
      },
      "outputs": [],
      "source": [
        "source_directory=\"../src/train/\"\n",
        "train_step = PythonScriptStep(\n",
        "    name=\"Train Model Step\",\n",
        "    script_name=\"train.py\",\n",
        "    compute_target=aml_compute,\n",
        "    source_directory=source_directory,\n",
        "    outputs=[models_data],\n",
        "    arguments=[\n",
        "        \"--model\",\n",
        "        models_data\n",
        "    ],\n",
        "    runconfig=run_config,\n",
        "    allow_reuse=True,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1670961576044
        }
      },
      "outputs": [],
      "source": [
        "source_directory=\"../src/evaluation/\"\n",
        "evaluate_step = PythonScriptStep(\n",
        "    name=\"Evaluate Model Step\",\n",
        "    script_name=\"eval.py\",\n",
        "    compute_target=aml_compute,\n",
        "    source_directory=source_directory,\n",
        "    inputs=[models_data],\n",
        "    arguments=[\n",
        "        \"--model_path\",\n",
        "        models_data,\n",
        "    ],\n",
        "    runconfig=run_config,\n",
        "    allow_reuse=False,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Registration step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "source_directory=\"../src/register/\"\n",
        "register_step = PythonScriptStep(\n",
        "    name=\"Registration Model Step\",\n",
        "    script_name=\"register.py\",\n",
        "    compute_target=aml_compute,\n",
        "    source_directory=source_directory,\n",
        "    inputs=[models_data],\n",
        "    arguments=[\n",
        "        \"--model_path\",\n",
        "        models_data,\n",
        "    ],\n",
        "    runconfig=run_config,\n",
        "    allow_reuse=False,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1670961750727
        }
      },
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import Pipeline\n",
        "train_step.run_after(preprocess_step)\n",
        "evaluate_step.run_after(train_step)\n",
        "register_step.run_after(evaluate_step)\n",
        "steps = [preprocess_step,train_step, evaluate_step,register_step]\n",
        "\n",
        "train_pipeline = Pipeline(workspace=ws, steps=steps)\n",
        "#train_pipeline._set_experiment_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created step Preprocessing Step [e44e04d5][a499580b-0362-4649-9672-de20520987dc], (This step is eligible to reuse a previous run's output)\n",
            "Created step Train Model Step [69e488a3][1f1baf0a-c474-4c27-a7b0-295d70cfb640], (This step is eligible to reuse a previous run's output)\n",
            "Created step Evaluate Model  [e42cb0e4][64f34c78-b057-4ce3-924d-245e92720fc7], (This step will run and generate new outputs)\n",
            "Created step Registration Model  [0e59775d][547e96e9-2d53-42ce-beb0-98f653f8c2ed], (This step will run and generate new outputs)\n",
            "Using data reference test_data for StepId [7aaed69e][ee17b8e7-aad7-4670-a604-b0b5a122ca4f], (Consumers of this data are eligible to reuse prior runs.)\n",
            "Published pipeline: preproc-train-register pipeline\n"
          ]
        }
      ],
      "source": [
        "train_pipeline.validate()\n",
        "published_pipeline = train_pipeline.publish(\n",
        "    name=\"preproc-train-register pipeline\",\n",
        "    description=\"Model training/retraining pipeline\"\n",
        ")\n",
        "print(f\"Published pipeline: {published_pipeline.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1670961582316
        }
      },
      "outputs": [],
      "source": [
        "#pipeline_run1 = Experiment(ws, 'titanic-pipeline').submit(train_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1671188685176
        }
      },
      "outputs": [],
      "source": [
        "#pipeline_run1.wait_for_completion(show_output=True)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "68cd809ea8153bc6204cc41955edea3b139b9aa4275a334cb56f84dd9247969a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
